# UniFlow containers
version: "3.7"

services:
  krakend:
    # user: root
    environment:
      - FC_ENABLE=1
      - FC_SETTINGS=/etc/krakend/settings
      - FC_PARTIALS=/etc/krakend/partials
      - FC_TEMPLATES=/etc/krakend/templates
      # - FC_OUT=/etc/krakend/output.json
    image: frinx/krakend:1.2.0

    volumes:
      - ${UF_CONFIG_PATH}/krakend:/etc/krakend:ro
      - ${UF_CONFIG_PATH}/krakend/tls/${KRAKEND_TLS_PROTOCOL}:/etc/krakend/partials/tls:ro
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    ports:
      - target: 8080
        published: ${KRAKEND_PORT}
        mode: host # This makes host to listen on 0.0.0.0:${KRAKEND_PORT} 
                   # It is the same as you would expect when publishing a port when not running in swarm mode. 
                   # It is NOT host networking in plain docker.
    deploy:
      placement:
        constraints:
          - node.hostname == ${CONSTRAINT_HOSTNAME}
      resources:
        limits:
          cpus: '0.25'              
          memory: 512M
          # cpu-shares: '' # https://docs.datadoghq.com/security_monitoring/default_rules/cis-docker-1.2.0-5.11/ , https://github.com/moby/moby/issues/36388
        reservations:
          cpus: '0.25'
          memory: 256M
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: continue
        monitor: 60s
        max_failure_ratio: 0.3
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    security_opt:
      - no-new-privileges:true




    # security_opt:
    #   - no-new-privileges:true
    # mem_limit: 512m
    # mem_reservation: 128M

  conductor-server:
    environment:
      - CONFIG_PROP=config.properties
    image: frinx/uniflow-conductor-server:1.0.0
    volumes:
      -  ${UF_CONFIG_PATH}/conductor/config.properties:/app/config/config.properties:ro
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    deploy:
      placement:
        constraints:
          - node.hostname == ${CONSTRAINT_HOSTNAME}
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '8.25'              
          memory: 12G
        reservations:
          cpus: '0.25'
          memory: 2G
    healthcheck:
      test: wget --spider localhost:8080/api/health
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 60s

    ulimits:
      nofile:
        soft: 65536
        hard: 65536


  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.7.1
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    environment:
      - "discovery.type=single-node"
      - "ES_JAVA_OPTS=-Xms512m -Xmx1024m"
    volumes:
      - uniflow_elastic_data:/usr/share/elasticsearch/data
      -  ${UF_CONFIG_PATH}/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
      -  ${UF_CONFIG_PATH}/elasticsearch/backup.sh:/usr/share/elasticsearch/backup.sh:ro
    healthcheck:
      test: curl -X GET 'localhost:9200/_cluster/health?wait_for_status=yellow&timeout=5s'
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    deploy:
      placement:
        constraints:
          - node.hostname == ${CONSTRAINT_HOSTNAME}
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.75'              
          memory: 1024M
        reservations:
          cpus: '0.5'
          memory: 1024M
    ulimits:
      nofile:
        soft: 65536
        hard: 65536

  dashboard:
    # TODO: make stable release and change 'latest' version to a stable one
    image: frinx/frinx-dashboard:latest
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    deploy:
      placement:
        constraints:
          - node.hostname == ${CONSTRAINT_HOSTNAME}
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.25'              
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test: curl --silent -o /dev/null  --write-out 'HTTPSTATUS:%{http_code}\n' -X GET 'http://127.0.0.1:5001' || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

    ulimits:
      nofile:
        soft: 65536
        hard: 65536


  postgresql:
    image: postgres:12.2
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    environment:
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_MULTIPLE_DATABASES=conductor,frinx,schellar
    healthcheck:
      test: pg_isready -U postgres
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    volumes:
      - uniflow_postgresql_data:/var/lib/postgresql/data
      - ${UF_CONFIG_PATH}/uniflow-postgres:/docker-entrypoint-initdb.d:ro
    deploy:
      placement:
        constraints:
          - node.hostname == ${CONSTRAINT_HOSTNAME}
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.25'              
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    ulimits:
      nofile:
        soft: 65536
        hard: 65536

  schellar:
    image: frinx/uniflow-schellar:1.9.2
    environment:
      - LOG_LEVEL=debug
      - CHECK_INTERVAL_SECONDS=10
      - CONDUCTOR_API_URL=http://conductor-server:8080/api
      - BACKEND=postgres
      - POSTGRES_MIGRATIONS_DIR=migrations
      - POSTGRES_DATABASE_URL=host=postgresql port=5432 user=postgres password=postgres database=schellar
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    deploy:
      placement:
        constraints:
          - node.hostname == ${CONSTRAINT_HOSTNAME}
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.25'              
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test: wget --spider -q conductor-server:8080/api/health && wget --spider -q 127.0.0.1:3000/schedule && nc -z postgresql:5432
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    ulimits:
      nofile:
        soft: 65536
        hard: 65536

  workflow-proxy:
    image: frinx/workflow-proxy:1.0.3
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    healthcheck:
      test: curl --silent --write-out 'HTTPSTATUS:%{http_code}' -X GET 'http://127.0.0.1:8088/probe/readiness' && curl --insecure --write-out 'HTTPSTATUS:%{http_code}' --silent --fail -X GET http://conductor-server:8080/api/health
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    deploy:
      placement:
        constraints:
          - node.hostname == ${CONSTRAINT_HOSTNAME}
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.25'              
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    ulimits:
      nofile:
        soft: 65536
        hard: 65536

  uniconfig-ui:
    image: frinx/uniconfig-ui:2.0.3
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    deploy:
      placement:
        constraints:
          - node.hostname == ${CONSTRAINT_HOSTNAME}
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.25'              
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    environment:
      - HTTPS=false
      - OPENSSL_COMMAND_TO_GEN_CERT=openssl req -nodes -new -x509 -days 360 -keyout key.pem -out cert.pem -subj "/C=US/ST=Oregon/L=Portland/O=Company Name/OU=Org/CN=www.example.com"
      - RENEW_PERIOD=30000
    healthcheck:
      test: curl --insecure --silent --write-out 'HTTPSTATUS:%{http_code}' -X GET 'http://127.0.0.1:4000'
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    ulimits:
      nofile:
        soft: 65536
        hard: 65536

  uniflow-ui:
    image: frinx/uniflow-ui:2.0.2
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    deploy:
      placement:
        constraints:
          - node.hostname == ${CONSTRAINT_HOSTNAME}
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.25'              
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test: curl --silent --write-out 'HTTPSTATUS:%{http_code}' -X GET 'http://127.0.0.1:3000'
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    ulimits:
      nofile:
        soft: 65536
        hard: 65536

volumes:
  uniflow_elastic_data:
    name: uniflow_elastic_data
  uniflow_postgresql_data:
    name: uniflow_postgresql_data

# docker network create -d overlay --opt encrypted frinx-machine

networks:
  default:    
    name: frinx-machine
    driver: overlay
    driver_opts:
      encrypted: "true"

